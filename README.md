# COMPILERZZZ

# CHOCOPY Scanner and Parser

This project implements a scanner and parser for the "CHOCOPY" programming language. The scanner is responsible for tokenizing the source code into CHOCOPY files, while the parser is responsible for checking the structure and grammar of the code.

## Requirements

-Python 3.x
- `re` library
- `sys` library
- `pyfiglet` library
- `colorama` library

## Facility

1. Clone this repository:

    ```bash
    git clone
2. Access the project directory:
-cd chocopy-scanner-parser
3. Optionally, it is recommended to create and activate a virtual environment for the project.
## Use
The project consists of two main files: scanner.py and parser.py. Here's how to use each of them:

### Scanner (scanner.py)
The scanner takes care of tokenizing the source code into CHOCOPY files. To run the scanner, use the following command:
- python scanner.py file.chocopy
Replace file.chocopy with the path to the file you want to scan. The scanner will display the list of generated tokens and any lexical errors found.
#### Usage
To use the scanner, follow these steps:

1. Import the necessary modules:

- import re
- from enum import Enum
- from collections import namedtuple
2. Define the TokenType enumeration class, which represents the different types of tokens:
 ```class TokenType(Enum):
    NEWLINE = 1
    INDENT = 2
    DEDENT = 3
    IDENTIFIER = 4
    KEYWORD = 5
    LITERAL = 6
    OPERATOR = 7
    DELIMITER = 8
    ERROR = 9
```
3. Define the Token named tuple, which represents a single token with its type, value, line number, and column:
```Token = namedtuple('Token', ['type', 'value', 'line', 'column'])```
4. Implement the tokenize function, which takes a filename as input and returns a list of tokens:
```def tokenize(filename):
    tokens = []
    # Read the file and store the lines
    with open(filename) as f:
        lines = f.readlines()

    line_number = 0
    indent_stack = [0]

    # Process each line
    for line in lines:
        line_number += 1
        indentation = len(line) - len(line.lstrip())

        # Ignore blank lines
        if indentation == len(line):
            continue

        line = line[indentation:]

        # Handle indentation
        if indentation > indent_stack[-1]:
            indent_stack.append(indentation)
            tokens.append(Token(TokenType.INDENT, " ", line_number, indentation))
        else:
            while indentation < indent_stack[-1]:
                indent_stack.pop()
                tokens.append(Token(TokenType.DEDENT, " ", line_number, indentation))

        # Tokenize the line
        token_pattern = re.compile(r"(\b(False|None|True|and|as|assert|async|await|break|class|continue|def|int|str|del|elif|else|except|finally|for|from|global|if|import|in|is|lambda|nonlocal|not|or|pass|raise|return|try|while|with|yield)\b)|([a-zA-Z][a-zA-Z0-9_]*)|([{}()[\]:,.])|(\b0\b|([1-9][0-9]*))|->|([=!<>%/*+\-]=?)|(\b0[0-9]+)|(\"(?:[^\"\\]|\\.)*\")")
        matches = token_pattern.findall(line)

        for match in matches:
            token_value = next(value for value in match if value)
            token_type = TokenType.KEYWORD if match[0] else TokenType.IDENTIFIER if match[2] else TokenType.DELIMITER if match[3] else TokenType.LITERAL

            tokens.append(Token(token_type, token_value, line_number, line.index(token_value) + 1))

    return tokens
```


### Parser (parser.py)
The parser is responsible for checking the structure and grammar of the source code in CHOCOPY files. To run the parser, use the following command:
- python parser.py file.chocopy
Replace file.chocopy with the path to the file you want to scan. The parser will display the tokens consumed and any syntax errors found.

#### Usage
To use the parser, you need to import the required modules and classes:
 ```import re
import sys
import pyfiglet
from colorama import Fore, Style
from enum import Enum
from collections import namedtuple
```
#### Tokenization
- The tokenize(filename) function is responsible for tokenizing the input file and returning a list of tokens. It uses regular expressions (re) to match patterns and identify different types of tokens. The supported token types are defined in the TokenType enum class.

- The tokenization process involves reading the lines from the input file, ignoring comments, calculating indentation levels, and applying the token pattern to each line. Tokens are created using the Token named tuple and added to the tokens list.

- The function also handles error detection for lexical errors and displays error messages. At the end of tokenization, it prints the total number of errors detected.
#### Parsing
- The Parser class is responsible for parsing the list of tokens generated by the tokenize() function. It initializes with the tokens and sets the current token to None. The next() method is used to consume tokens one by one.

- The error(message) method is used to handle syntax errors. It appends the error message to the errors list and moves to the next token.

- The parser implements various production rules for parsing the Python code according to the Python grammar. Each rule is defined as a method in the Parser class.
* Program(): The entry point for the parser. It calls the DefList() and StatementList() methods.

* DefList(): Parses function definitions (def) by repeatedly calling the Def() method.

* Def(): Parses a function definition, including the function name, parameter list, return type, and function body.

* TypedVar(): Parses a typed variable declaration, including the variable name and its type.

* Type(): Parses variable types, which can be either "int", "str", or a list type.

* TypedVarList(): Parses a list of typed variables by repeatedly calling the TypedVar() method.

* Return(): Parses the return type of a function, if specified.

* Block(): Parses a block of statements, including handling indentation levels.

* StatementList(): Parses a list of statements by repeatedly calling the Statement() method.

* Statement(): Parses different types of statements, including if, while, for, and return statements.

* SimpleStatement(): Parses simple statements, which can be an expression or an assignment.

* Expr(): Parses expressions by calling the orExpr() method.

* ... (Other production rules)

The match(token_type, token_value=None) method is used to match the current token with an expected token type and value. If the match is successful, it moves to the next token. Otherwise, it reports a syntax error.

#### Error Handling
The parser includes error handling for syntax errors. Whenever an error is encountered, an error message is appended to the errors list. The error messages contain information about the location of the error (line number) and a description of the error.
#### Example Usage
To use the parser, you can follow these steps:
1. Tokenize the input file:
```tokens = tokenize("input_file.py")```
2. Create an instance of the parser:
```parser = Parser(tokens)```
3. Start parsing the code:
```parser.Program()```
4. Check for errors:

```if parser.errors:
    print("Syntax errors found:")
    for error in parser.errors:
        print(error)
else:
    print("No syntax errors found.")
```
Please note that this parser is a simplified implementation and may not handle all Python grammar rules or edge cases. It serves as a starting point for understanding the tokenization and parsing process.

## License

This project is licensed under the MIT License. For details, see the [LICENSE](LICENSE) file.

## Participants
-Carlos Gabriel Morales Umasi

-Joey Patrick Flores Davila
